<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mechanistic Interpretability - Tahereh Toosi</title>
    <style>
        :root {
            --name-color: #ff6781;         /* Salmon pink */
            --section-title-color: #80cbc4; /* Teal */
        }
        body {
            font-family: 'Open Sans', Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            color: #e0e0e0;
            background-color: #1e1e1e;
        }
        h1, h2, h3 {
            font-family: 'Montserrat', Arial, sans-serif;
            color: #66ccff;
        }
        h1 {
            margin-bottom: 10px;
            color: var(--name-color);
        }
        h2 {
            color: var(--section-title-color);
            margin-top: 40px;
        }
        .back-link {
            display: inline-block;
            margin-bottom: 30px;
            padding: 10px 20px;
            background-color: #2a2a2a;
            color: var(--section-title-color);
            text-decoration: none;
            border-radius: 4px;
            transition: background-color 0.3s;
        }
        .back-link:hover {
            background-color: #333;
        }
        .paper-info {
            background-color: #2a2a2a;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .paper-title {
            font-size: 1.4em;
            font-weight: bold;
            color: #fff;
            margin-bottom: 15px;
        }
        .paper-authors {
            color: #b0b0b0;
            font-style: italic;
            margin-bottom: 10px;
        }
        .paper-venue {
            color: var(--section-title-color);
            font-weight: 500;
        }
        .abstract {
            background-color: #2a2a2a;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid var(--section-title-color);
        }
        .abstract h3 {
            margin-top: 0;
            color: var(--section-title-color);
        }
        .content-section {
            margin: 30px 0;
        }
        .content-section h3 {
            color: var(--section-title-color);
            margin-top: 30px;
        }
        .content-section p {
            margin: 15px 0;
        }
        .figure-container {
            text-align: center;
            margin: 30px 0;
        }
        .figure-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.3);
        }
        .figure-caption {
            margin-top: 10px;
            font-style: italic;
            color: #b0b0b0;
            font-size: 0.9em;
        }
        .highlight-box {
            background-color: #2a2a2a;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid var(--name-color);
        }
        .highlight-box h4 {
            color: var(--name-color);
            margin-top: 0;
        }
        .method-steps {
            background-color: #2a2a2a;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }
        .method-steps h4 {
            color: var(--section-title-color);
            margin-top: 0;
        }
        .method-steps ol {
            padding-left: 20px;
        }
        .method-steps li {
            margin: 10px 0;
        }
        .tags {
            display: flex;
            gap: 10px;
            margin: 20px 0;
        }
        .tag {
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.8em;
            font-weight: 500;
        }
        .tag.neuroscience {
            background-color: #10b981;
            color: #ffffff;
        }
        .tag.ai {
            background-color: #FDD7E4;
            color: #333333;
        }
        .tag.interpretability {
            background-color: #8B5CF6;
            color: #ffffff;
        }
        .equation {
            background-color: #2a2a2a;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            text-align: center;
            font-family: 'Courier New', monospace;
            color: #fff;
        }
        .key-findings {
            background-color: #2a2a2a;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #10b981;
        }
        .key-findings h4 {
            color: #10b981;
            margin-top: 0;
        }
        .key-findings ul {
            padding-left: 20px;
        }
        .key-findings li {
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <a href="../index.html" class="back-link">← Back to Research</a>
    
    <h1>Mechanistic Interpretability: Network-Level Analysis</h1>
    
    <div class="paper-info">
        <div class="paper-title">Interpretability at the Network Level: Prior-Guided Drift Diffusion for Neural Circuit Analysis</div>
        <div class="paper-authors">Tahereh Toosi & Kenneth D. Miller</div>
        <div class="paper-venue">NeurIPS 2025 (Under Review)</div>
    </div>

    <div class="abstract">
        <h3>Abstract</h3>
        <p>Interpretability at the neuron level has provided valuable insights into how individual units respond to specific features and patterns. To advance interpretability at the network level, we propose treating networks as generative models to probe their learned statistical priors. We introduce Prior-Guided Drift Diffusion (PGDD), which accesses the implicit statistical structure networks acquire during training. PGDD iteratively refines inputs according to the network's learned priors, essentially probing what patterns emerge from the network's internal statistical knowledge. For adversarially robust networks, this leverages implicit denoising operators shaped by robust training. For standard networks, our extension uses gradient smoothing techniques to stabilize the generative process. Applying this method during early training reveals that networks appear to acquire rich semantic representations well before achieving reliable classification performance. This demonstrates a dissociation between internal representation learning and classification performance, where networks develop structured knowledge before they can reliably use it. Our training-free approach provides direct access to this latent representational structure in the models we tested.</p>
    </div>

    <div class="tags">
        <span class="tag neuroscience">Neuroscience</span>
        <span class="tag ai">AI</span>
        <span class="tag interpretability">Interpretability</span>
    </div>

    <div class="content-section">
        <h2>Research Overview</h2>
        <p>This work addresses a fundamental challenge in AI interpretability: understanding what concepts a network has learned at the population level, not just individual neuron responses. While neuron-level analysis has provided valuable insights, understanding network-level knowledge—what populations of neurons collectively know—remains challenging.</p>
        
        <p>Current approaches have significant limitations: external generative models impose their own inductive biases, while methods requiring additional training face practical challenges including hyperparameter sensitivity, seed-dependent instability, and poor downstream performance.</p>
    </div>

    <div class="content-section">
        <h2>Prior-Guided Drift Diffusion (PGDD)</h2>
        
        <div class="highlight-box">
            <h4>Core Innovation</h4>
            <p>Instead of asking "what activates this neuron?", PGDD asks "what patterns can this network generate from its learned statistical knowledge?" This treats networks as implicit generative models to probe their learned priors.</p>
        </div>

        <div class="method-steps">
            <h4>How PGDD Works</h4>
            <ol>
                <li><strong>Initialize</strong> with noise patterns (e.g., Perlin noise)</li>
                <li><strong>Iteratively refine</strong> inputs according to the network's learned statistical regularities</li>
                <li><strong>Access implicit denoisers</strong> shaped by adversarial training</li>
                <li><strong>Generate patterns</strong> that reflect the network's internal knowledge</li>
            </ol>
        </div>

        <div class="equation">
            <strong>PGDD Objective:</strong><br>
            L_PGDD(ẋ) = ||r_ℓ(ẋ) - sg(r_ℓ(ẋ + ε))||₂²
        </div>

        <p>Where r_ℓ(·) are representations at layer ℓ, ε ~ N(0, σ²I), and sg(·) stops gradients through the reference.</p>
    </div>

    <div class="content-section">
        <h2>Key Findings</h2>
        
        <div class="key-findings">
            <h4>Rapid Semantic Learning</h4>
            <ul>
                <li>Networks acquire semantic structure (e.g., bird-like features) within just a few epochs</li>
                <li>This occurs well before classification accuracy improves</li>
                <li>Reveals a dissociation between internal representation learning and external performance</li>
            </ul>
        </div>

        <div class="figure-container">
            <img src="../project_generative_inference/imagination_demo_combined_slow.gif" alt="PGDD Concept" style="width: 100%; max-width: 800px;">
            <div class="figure-caption">
                <strong>Network-level interpretability through implicit generative operators.</strong> PGDD shifts from neuron-level analysis to network-level understanding by running networks as generative models to probe learned priors. Across different noise initializations, PGDD consistently produces recognizable patterns with semantic features, suggesting rapid semantic learning in early training.
            </div>
        </div>

        <div class="key-findings">
            <h4>Early Training Insights</h4>
            <ul>
                <li>By epoch 4, networks consistently generate bird-like patterns across different noise seeds</li>
                <li>Per-category accuracy analysis shows preferential learning of avian categories despite poor overall performance</li>
                <li>4 of the top 10 performing categories are bird classes at epoch 4</li>
            </ul>
        </div>
    </div>

    <div class="content-section">
        <h2>Extension to Standard Networks (sPGDD)</h2>
        
        <p>For standard networks that lack the well-structured J^T J operator shaped by adversarial training, we developed <em>sPGDD</em> (smooth PGDD). The core idea is to stabilize the update step by smoothing the gradients at each iteration, rather than relying on a single noisy gradient estimate.</p>
        
        <div class="method-steps">
            <h4>sPGDD Process</h4>
            <ol>
                <li>Fix the noisy reference representation r(x+ε) once at the start</li>
                <li>At each iteration, compute multiple gradients with respect to independently sampled noise perturbations</li>
                <li>Average the gradients to reduce variance and suppress spurious noise-sensitive directions</li>
                <li>Emphasize stable prior information embedded in the network</li>
            </ol>
        </div>
    </div>

    <div class="content-section">
        <h2>Biological Connections</h2>
        
        <p>PGDD extends the broader principle of Generative Inference, which proposes that perception is an active inferential process shaped by the integration of sensory inputs with learned priors. In biological vision, feedback signals are recruited especially when perception cannot rely on clear feedforward cues alone—for example, in cases of ambiguous, incomplete, or noisy inputs.</p>
        
        <p>This feedback-driven integration explains hallmark perceptual effects such as:</p>
        <ul>
            <li>Delayed neural responses to illusory contours</li>
            <li>Laminar-specific activation patterns in early visual cortex</li>
            <li>Flexible interpretation of ambiguous stimuli</li>
            <li>Figure-ground segregation and Gestalt principles</li>
        </ul>
    </div>

    <div class="content-section">
        <h2>Implications</h2>
        
        <div class="highlight-box">
            <h4>For AI Safety and Understanding</h4>
            <p>PGDD provides a way to track training trajectories, uncover hidden biases in learned representations, and assess how priors shape model behavior. These capabilities may inform both scientific inquiry and the safety of deployed AI systems.</p>
        </div>

        <div class="highlight-box">
            <h4>For Learning Theory</h4>
            <p>The rapid emergence of semantic categories like birds—confirmed by per-category performance analysis—suggests networks acquire internal knowledge faster than external metrics indicate. This aligns with theories of rapid concept acquisition and provides new insights into learning dynamics.</p>
        </div>
    </div>

    <div class="content-section">
        <h2>Future Directions</h2>
        
        <p>Future work should extend PGDD to other architectures and domains, exploring its role as both an analytical tool and a diagnostic instrument for emerging reasoning models. The method offers new possibilities for understanding increasingly sophisticated AI systems and their internal knowledge structures.</p>
    </div>

    <div class="tags">
        <span class="tag neuroscience">Neuroscience</span>
        <span class="tag ai">AI</span>
        <span class="tag interpretability">Interpretability</span>
    </div>

</body>
</html>
