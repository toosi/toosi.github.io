<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Generative Inference - Tahereh Toosi</title>
    <style>
        :root {
            /* Light theme (default) */
            --name-color: #fc5203;         /* Warm orange-red */
            --section-title-color: #006699; /* Teal-blue */
            --text-primary: #1e293b;        /* Slate-800 */
            --text-secondary: #475569;      /* Slate-600 */
            --text-tertiary: #64748b;       /* Slate-500 */
            --accent-color: #0088aa;        /* Teal-blue accent */
            --border-color: #e2e8f0;        /* Slate-200 */
            --bg-subtle: #f8fafc;          /* Slate-50 */
            --bg-primary: #ffffff;         /* White background */
            --bg-secondary: #f9f9f9;       /* Light gray background */
            --text-body: #333333;          /* Dark text */
            --link-color: #5b8a9f;         /* Muted teal-blue for links */
        }
        
        /* Dark theme */
        @media (prefers-color-scheme: dark) {
            :root {
                --name-color: #ff6b3d;         /* Brighter orange-red for dark theme */
                --section-title-color: #4dd0e1; /* Lighter teal-blue */
                --text-primary: #e2e8f0;        /* Light slate */
                --text-secondary: #cbd5e1;      /* Lighter gray */
                --text-tertiary: #94a3b8;       /* Medium gray */
                --accent-color: #5dd3e8;        /* Brighter teal accent */
                --border-color: #334155;        /* Dark slate borders */
                --bg-subtle: #1e293b;          /* Dark slate background */
                --bg-primary: #0f172a;         /* Very dark blue background */
                --bg-secondary: #1e293b;       /* Dark slate for cards */
                --text-body: #e2e8f0;          /* Light text */
                --link-color: #5dd3e8;          /* Brighter link color for dark theme */
            }
        }
        
        body {
            font-family: 'Open Sans', Verdana, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: var(--text-body);
            background-color: var(--bg-primary);
            transition: background-color 0.3s ease, color 0.3s ease;
        }
        .back-link {
            display: inline-block;
            margin-bottom: 20px;
            color: var(--section-title-color);
            text-decoration: none;
            transition: color 0.2s ease;
        }
        .back-link:hover {
            text-decoration: underline;
            color: var(--accent-color);
        }
        h1, h2, h3 {
            font-family: 'Montserrat', Arial, sans-serif;
            color: var(--section-title-color);
        }
        h1 {
            color: var(--name-color);
            margin-bottom: 20px;
        }
        .content {
            background-color: var(--bg-secondary);
            padding: 24px;
            border-radius: 12px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            border: 1px solid var(--border-color);
            transition: background-color 0.3s ease, border-color 0.3s ease;
        }
        @media (prefers-color-scheme: dark) {
            .content {
                box-shadow: 0 2px 4px rgba(0,0,0,0.3);
            }
        }
        .publications {
            margin-top: 30px;
        }
        .publication {
            margin-bottom: 20px;
            border-left: 3px solid var(--section-title-color);
            padding-left: 18px;
            padding-top: 6px;
            padding-bottom: 6px;
            background: linear-gradient(to right, rgba(0, 102, 153, 0.02), transparent);
            border-radius: 0 4px 4px 0;
            transition: all 0.3s ease;
        }
        @media (prefers-color-scheme: dark) {
            .publication {
                background: linear-gradient(to right, rgba(77, 208, 225, 0.05), transparent);
            }
        }
        .publication p {
            margin: 5px 0;
        }
        .publication .title {
            font-weight: 600;
            color: var(--text-primary);
        }
        .publication .title a {
            color: var(--link-color);
            text-decoration: underline;
            transition: color 0.2s ease;
        }
        .publication .title a:hover {
            color: var(--accent-color);
        }
        .publication .authors {
            font-style: italic;
            color: var(--text-secondary);
        }
        .publication .venue {
            color: var(--text-tertiary);
        }
        p {
            color: var(--text-body);
        }
        a[style*="color: #66ccff"] {
            color: var(--link-color) !important;
        }
    </style>
</head>
<body>
    <a href="../index.html" class="back-link">← Back to Home</a>
    
    <div class="content">
        <h1>Generative inference: a unifying principle for the integration of learned priors in natural and artificial intelligence</h1>
        
        <p>Leveraging learned knowledge to interpret ambiguous or unusual sensory inputs is a hallmark of biological intelligence, yet artificial systems typically cannot utilize their learned priors beyond narrowly defined training experiences. I developed "Generative Inference", a computational principle that enables any artificial neural network to access and deploy its implicitly learned statistical regularities through feedback pathways. Applied to vision, we demonstrate that Generative Inference accounts for both perceptual experiences and neural signatures across a spectrum of previously disparate phenomena—from illusory contours to figure-ground segregation, brightness illusions, and even hallucination-like pattern formation—all within systems capable of rapid object recognition.</p>

        <h2>Project Page</h2>
        <p><a href="../project_generative_inference/generative_inference_page.html" style="color: #66ccff; text-decoration: underline;">Interactive demonstrations and detailed project information</a></p>

        <h2>Related Publications</h2>
        <div class="publications">
            <div class="publication">
                <p class="title"><a href="https://www.biorxiv.org/content/10.1101/2025.10.21.683535v2" target="_blank" style="color: #66ccff; text-decoration: underline;">Generative inference unifies feedback processing for learning and perception in natural and artificial vision</a></p>
                <p class="authors">Toosi, T., & Miller, K. D.</p>
                <p class="venue">bioRxiv (2025)</p>
            </div>
            
            <div class="publication">
                <p class="title">Unifying Gestalt Principles Through Inference-Time Prior Integration</p>
                <p class="authors">Toosi, T., & Miller, K. D.</p>
                <p class="venue">Interpreting Cognition in Deep Learning Models Workshop, Neural Information Processing Systems (NeurIPS) (2025)</p>
            </div>
            
            <div class="publication">
                <p class="title">Illusions as features: the generative side of recognition</p>
                <p class="authors">Toosi, T., & Miller, K. D.</p>
                <p class="venue">Workshop on Scientific Methods for Understanding Deep Learning, Advances in Neural Information Processing Systems (NeurIPS) (2024)</p>
            </div>
            
            <div class="publication">
                <p class="title">Generative inference in object recognition models—A unifying framework for discriminative and generative computations in vision</p>
                <p class="authors">Toosi, T., & Miller, K. D.</p>
                <p class="venue">From Neuroscience to Artificially Intelligent Systems (2024)</p>
            </div>
            
            <div class="publication">
                <p class="title">Generative perceptual inference in deep neural network models of object recognition induces illusory contours and shapes</p>
                <p class="authors">Toosi, T., & Miller, K. D.</p>
                <p class="venue">Cognitive Computational Neuroscience (CCN) (2024)</p>
            </div>
        </div>

        <h2>Invited Talks</h2>
        <div class="publications">
            <div class="publication">
                <p class="title">Intrinsic alignment to natural intelligence by integrating learned priors</p>
                <p class="venue">Gatsby Tri-Center Annual Meeting, University College London (2025)</p>
            </div>
            
            <div class="publication">
                <p class="title">Generative inference in object recognition models—A unifying framework for discriminative and generative computations in vision</p>
                <p class="venue">Johns Hopkins Kavli Neuroscience Discovery Institute, Baltimore (2025)</p>
            </div>
            
            <div class="publication">
                <p class="title">Generative inference in object recognition models—A unifying framework for discriminative and generative computations in vision</p>
                <p class="venue">TigerBrain Research Symposium, Princeton University (2024)</p>
            </div>
    
            <div class="publication">
                <p class="title">Generative inference in object recognition models—A unifying framework for discriminative and generative computations in vision</p>
                <p class="venue">From Neuroscience to Artificially Intelligent Systems, Cold Spring Harbor Laboratory (2024)</p>
            </div>
            <div class="publication">
                <p class="title">Generative perceptual inference in deep neural network models of object recognition induces illusory shapes</p>
                <p class="venue">Swartz Foundation Meeting, University of Washington (2024)</p>
            </div>
            <div class="publication">
                <p class="title">Emergence of illusory contours in robust deep neural networks by accumulation of implicit priors</p>
                <p class="venue">Object Recognition: Models, Vision Science Society Meeting, St. Pete Beach Florida (2024)</p>
            </div>
            <div class="publication">
                <p class="title">Cortical computations underlying the integration of perceptual priors and sensory processing</p>
                <p class="venue">Brain Science External Postdoc Seminar Series, Brown University (2024)</p>
            </div>
            <div class="publication">
                <p class="title">Harnessing feedback pathways: Integrating perceptual priors in sensory processing</p>
                <p class="venue">SYNAPSES Seminar Series, Yale University (2024)</p>
            </div>
        </div>


        <section id="public-demos" class="public-demos section">
            <h2>Public Demos</h2>
            <div class="project">
                <img src="../assets/gifs/Demo_night.png" alt="Demo Night - Dementia Self-Perception" style="width:100%;max-width:500px;display:block;margin:20px auto 0 auto;border-radius:8px;box-shadow:0 2px 8px rgba(0,0,0,0.3);" />
                <p style="text-align:center;margin-top:10px;font-style:italic;">"How do I see myself if I develop dementia?"<br>Presented at Vision Science Society, Demo night, 2024</p>
            </div>
        </section>
    </div>
</body>
</html> 