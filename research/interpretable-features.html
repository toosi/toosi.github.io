<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interpretable features to identify neural representations - Tahereh Toosi</title>
    <style>
        :root {
            --name-color: #ff6781;         /* Salmon pink */
            --section-title-color: #80cbc4; /* Teal */
        }
        body {
            font-family: 'Open Sans', Verdana, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #ffffff;
            background-color: #000000;
        }
        h1, h2, h3 {
            font-family: 'Montserrat', Arial, sans-serif;
            color: var(--section-title-color);
        }
        h1 {
            color: var(--name-color);
        }
        a {
            color: var(--section-title-color);
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .back-link {
            display: inline-block;
            margin-bottom: 20px;
        }
        .publication {
            margin: 20px 0;
            padding: 15px;
            background-color: #1a1a1a;
            border-radius: 8px;
        }
        .publication .title {
            font-weight: bold;
            color: #ffffff;
        }
        .publication .authors {
            font-style: italic;
            color: #b0b0b0;
        }
        .publication .venue {
            color: var(--section-title-color);
        }
    </style>
</head>
<body>
    <a href="../index.html" class="back-link">‚Üê Back to Home</a>
    
    <h1>Interpretable features to identify neural representations</h1>
    
    <h2>Project Overview</h2>
    <p>
        Understanding how neural networks represent and process information is crucial for both neuroscience and artificial intelligence. This project focuses on developing interpretable features to identify and analyze neural representations across different systems. We aim to bridge the gap between biological and artificial neural networks by creating tools that can reveal the underlying computational principles and organizational patterns in both domains.
    </p>
    <!-- <img src="../assets/gifs/MSA_preview.png" alt="MSA interpretable features preview" style="width:100%;max-width:800px;display:block;margin:20px auto 30px auto;border-radius:8px;box-shadow:0 2px 8px rgba(0,0,0,0.3);" /> -->

    <h2>Research Goals</h2>
    <ul>
        <li>Develop interpretable features for analyzing neural representations</li>
        <li>Create tools for comparing representations across different neural systems</li>
        <li>Understand the relationship between neural representations and computational function</li>
        <li>Apply these methods to both biological and artificial neural networks</li>
        <li>Identify common principles in neural representation across different systems</li>
    </ul>

    <h2>Research Approach</h2>
    <p>
        We combine techniques from machine learning, neuroscience, and computational modeling to develop interpretable features that can reveal the structure and organization of neural representations. Our approach involves analyzing both biological neural data and artificial neural networks to identify common patterns and principles. We focus on creating tools that are both theoretically grounded and practically useful for understanding neural computation.
    </p>

    <h2>Key Components</h2>
    <ul>
        <li>Development of interpretable feature extraction methods</li>
        <li>Analysis of neural representations in biological systems</li>
        <li>Comparison of representations across different neural networks</li>
        <li>Validation of interpretability methods through controlled experiments</li>
        <li>Application to both neuroscience and AI research questions</li>
    </ul>

    <h2>Related Publications</h2>
    <div class="publication">
        <p class="title"><a href="https://2023.ccneuro.org/view_paper5d2e.html?PaperNum=1349" target="_blank">Object-enhanced and object-centered representations across primate ventral visual cortex</a></p>
        <p class="authors">Toosi, T., Kriegeskorte, N., & Issa, E. B.</p>
        <p class="venue">Cognitive Computational Neuroscience (CCN) (2023)</p>
    </div>

    <div class="publication">
        <p class="title"><a href="https://openreview.net/forum?id=tOW3IWHw8G" target="_blank">Representational constraints underlying similarity between task-optimized neural systems</a></p>
        <p class="authors">Toosi, T.</p>
        <p class="venue">Unifying Representations in Neural Models Workshop, Neural Information Processing Systems (NeurIPS) (2023)</p>
    </div>

    <h2>Invited Talks</h2>
    <div class="publication">
        <p class="title"><a href="https://www.youtube.com/watch?v=aT2rBlSrfP0" target="_blank">Can images predict neural patterns better than Deep Nets?</a></p>
        <p class="venue">ICBINB Workshop, Cosyne Meeting, Lisbon, Portugal (2024)</p>
    </div>

    <div class="publication">
        <p class="title">Uncovering the evolution of neural representations in the ventral visual stream</p>
        <p class="venue">Neuroscience and Artificial Intelligence Laboratory (NeuroAILab), Stanford University (2023)</p>
    </div>

    <div class="publication">
        <p class="title">Interpretable intermediate representations in primate ventral visual cortex</p>
        <p class="venue">Visual Inference Lab, Columbia University (2023)</p>
    </div>

    <!-- <h2>Related Publications</h2>
    <div class="publication">
        <p class="title">Interpretable features for neural representation analysis</p>
        <p class="authors">Toosi, T.</p>
        <p class="venue">In preparation</p>
    </div> -->
</body>
</html> 