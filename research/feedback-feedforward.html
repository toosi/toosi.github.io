<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feedback Feedforward Alignment - Tahereh Toosi</title>
    <style>
        :root {
            /* Light theme (default) */
            --name-color: #fc5203;         /* Warm orange-red */
            --section-title-color: #006699; /* Teal-blue */
            --text-primary: #1e293b;        /* Slate-800 */
            --text-secondary: #475569;      /* Slate-600 */
            --text-tertiary: #64748b;       /* Slate-500 */
            --accent-color: #0088aa;        /* Teal-blue accent */
            --border-color: #e2e8f0;        /* Slate-200 */
            --bg-subtle: #f8fafc;          /* Slate-50 */
            --bg-primary: #ffffff;         /* White background */
            --bg-secondary: #f9f9f9;       /* Light gray background */
            --text-body: #333333;          /* Dark text */
            --link-color: #5b8a9f;         /* Muted teal-blue for links */
        }
        
        /* Dark theme */
        @media (prefers-color-scheme: dark) {
            :root {
                --name-color: #ff6b3d;         /* Brighter orange-red for dark theme */
                --section-title-color: #4dd0e1; /* Lighter teal-blue */
                --text-primary: #e2e8f0;        /* Light slate */
                --text-secondary: #cbd5e1;      /* Lighter gray */
                --text-tertiary: #94a3b8;       /* Medium gray */
                --accent-color: #5dd3e8;        /* Brighter teal accent */
                --border-color: #334155;        /* Dark slate borders */
                --bg-subtle: #1e293b;          /* Dark slate background */
                --bg-primary: #0f172a;         /* Very dark blue background */
                --bg-secondary: #1e293b;       /* Dark slate for cards */
                --text-body: #e2e8f0;          /* Light text */
                --link-color: #5dd3e8;          /* Brighter link color for dark theme */
            }
        }
        
        body {
            font-family: 'Open Sans', Verdana, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: var(--text-body);
            background-color: var(--bg-primary);
            transition: background-color 0.3s ease, color 0.3s ease;
        }
        .back-link {
            display: inline-block;
            margin-bottom: 20px;
            color: var(--section-title-color);
            text-decoration: none;
            transition: color 0.2s ease;
        }
        .back-link:hover {
            text-decoration: underline;
            color: var(--accent-color);
        }
        h1, h2, h3 {
            font-family: 'Montserrat', Arial, sans-serif;
            color: var(--section-title-color);
        }
        h1 {
            color: var(--name-color);
            margin-bottom: 20px;
        }
        .content {
            background-color: var(--bg-secondary);
            padding: 24px;
            border-radius: 12px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            border: 1px solid var(--border-color);
            transition: background-color 0.3s ease, border-color 0.3s ease;
        }
        @media (prefers-color-scheme: dark) {
            .content {
                box-shadow: 0 2px 4px rgba(0,0,0,0.3);
            }
        }
        .publications {
            margin-top: 30px;
        }
        .publication {
            margin-bottom: 20px;
            border-left: 3px solid var(--section-title-color);
            padding-left: 18px;
            padding-top: 6px;
            padding-bottom: 6px;
            background: linear-gradient(to right, rgba(0, 102, 153, 0.02), transparent);
            border-radius: 0 4px 4px 0;
            transition: all 0.3s ease;
        }
        @media (prefers-color-scheme: dark) {
            .publication {
                background: linear-gradient(to right, rgba(77, 208, 225, 0.05), transparent);
            }
        }
        .publication p {
            margin: 5px 0;
        }
        .publication .title {
            font-weight: 600;
            color: var(--text-primary);
        }
        .publication .title a {
            color: var(--link-color);
            text-decoration: underline;
            transition: color 0.2s ease;
        }
        .publication .title a:hover {
            color: var(--accent-color);
        }
        .publication .authors {
            font-style: italic;
            color: var(--text-secondary);
        }
        .publication .venue {
            color: var(--text-tertiary);
        }
        p {
            color: var(--text-body);
        }
        a[style*="color: #66ccff"] {
            color: var(--link-color) !important;
        }
    </style>
</head>
<body>
    <a href="../index.html" class="back-link">← Back to Home</a>
    
    <div class="content">
        <h1>Feedback Feedforward Alignment</h1>
        <h2>as a more biologically plausible learning algorithm alternative to backpropagation of errors, allows for flexible inference</h2>
        <p>Cortical feedback connections underlie a rich palette of perceptual phenomena—from imagination and hallucination to occlusion resolution—which motivated our framework. We introduce Feedback–Feedforward Alignment (FFA), a biologically grounded learning algorithm in which:
<ul>
<li>Feedforward pathways optimize a discriminative objective (e.g., classification loss),</li>
<li>Feedback pathways optimize an autoencoder-style reconstruction objective,</li>
<li>Both streams serve as mutual credit-assignment graphs, aligning their updates through purely local synaptic rules and thus bypassing the weight-transport problem.</li>
</ul>
During inference, trained forward and feedback connections run in a loop—iteratively refining activations based on reconstruction errors—to produce emergent generative functions such as denoising, de-occlusion, hallucination, and vivid mental imagery. Across MNIST and CIFAR-10 benchmarks, FFA achieves strong performance , exhibits robustness to input noise, and replicates flexible visual inference without any backpropagation.</p>

        

        <h2>Related Publications</h2>
        <div class="publications">
            <div class="publication">
                <p class="title"><a href="https://papers.nips.cc/paper_files/paper/2023/file/b29ec434e049fb96f3c4245a405ee976-Supplemental-Conference.pdf" target="_blank">Brain-like flexible visual inference by harnessing feedback-feedforward alignment</a></p>
                <p class="authors">Toosi, T., & Issa, E. B.</p>
                <p class="venue">Advances in Neural Information Processing Systems (NeurIPS) (2023)</p>
            </div>
        </div>

        <h2>Talks</h2>
        <div class="publications">
            <div class="publication">
                <p class="title">Symbiotic learning of feedforward and feedback networks</p>
                <p class="venue">From Neuroscience to Artificially Intelligent Systems, Cold Spring Harbor Laboratory, 2020</p>
            </div>
        </div>

        <h2>Poster Presentation</h2>
        <div class="publications">
            <div class="publication">
                <p class="title"><a href="../assets/posters/FFA_poster.png" target="_blank">Feedback–Feedforward Alignment (FFA) Poster</a></p>
                <p class="venue">Presented at NeurIPS 2023</p>
                <a href="../assets/posters/FFA_poster.png" target="_blank">
                    <img src="../assets/posters/FFA_poster.png" alt="FFA Poster Preview" style="max-width:100%;height:auto;border-radius:8px;box-shadow:0 2px 8px rgba(0,0,0,0.3);margin-top:10px;" />
                </a>
            </div>
        </div>
    </div>
</body>
</html> 