<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tahereh Toosi - Academic Website</title>
    <style>
        :root {
            --name-color: #ff6781;         /* Salmon pink */
            --section-title-color: #80cbc4; /* Teal */
        }
        body {
            font-family: 'Open Sans', Verdana, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #e0e0e0;
            background-color: #1e1e1e;
        }
        header {
            display: flex;
            align-items: center;
            margin-bottom: 30px;
        }
        .profile-img {
            width: 200px;
            border-radius: 50%;
            margin-right: 30px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
        }
        .header-content {
            flex: 1;
        }
        h1, h2, h3 {
            font-family: 'Montserrat', Arial, sans-serif;
            color: #66ccff;
        }
        h1 {
            margin-bottom: 10px;
            color: var(--name-color);
        }
        .projects {
            margin-top: 40px;
        }
        .projects h2 {
            color: var(--section-title-color);
        }
        .project {
            margin-bottom: 30px;
            padding: 20px;
            border-radius: 8px;
            background-color: #2a2a2a;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .project h3 {
            margin-top: 0;
        }
        .project h4 {
            color: #b0b0b0;
            font-family: 'Open Sans', Verdana, sans-serif;
            font-size: 1.1em;
            margin: 10px 0;
            font-style: italic;
        }
        .project .tags {
            display: flex;
            gap: 10px;
            margin: 15px 0 0 0;
        }
        .project .tag {
            padding: 3px 6px;
            border-radius: 4px;
            font-size: 0.7em;
            font-family: 'Open Sans', Verdana, sans-serif;
            font-weight: 500;
        }
        .project .tag.neuroscience {
            background-color: #10b981; /* Emerald */
            color: #ffffff;
        }
        .project .tag.ai {
            background-color: #FDD7E4; /* Pale pink */
            color: #333333;
        }
        .project a {
            color: var(--section-title-color);
            text-decoration: underline;
        }
        .project a:hover {
            text-decoration: none;
        }
        .project .gif-container {
            display: flex;
            gap: 20px;
            margin: 15px 0;
        }
        .project img {
            width: 200px;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.2);
        }
        .publications {
            margin-top: 40px;
        }
        .publication {
            margin-bottom: 20px;
            border-left: 3px solid var(--section-title-color);
            padding-left: 15px;
        }
        .publication p {
            margin: 5px 0;
        }
        .publication .title {
            font-weight: bold;
        }
        .publication .authors {
            font-style: italic;
        }
        .publication .venue {
            color: #b0b0b0;
        }
        .publications h2 {
            color: var(--section-title-color);
        }
        .nav {
            display: flex;
            justify-content: center;
            margin: 30px 0;
            gap: 20px;
        }
        .nav a {
            padding: 10px 20px;
            background-color: #2a2a2a;
            color: var(--section-title-color);
            text-decoration: none;
            border-radius: 4px;
            transition: background-color 0.3s;
        }
        .nav a:hover, .nav a.active {
            background-color: #333;
        }
        .section {
            display: none;
        }
        .section.active {
            display: block;
        }
        .social-icons {
            display: flex;
            margin-top: 15px;
            gap: 15px;
        }
        .social-icon {
            width: 24px;
            height: 24px;
        }
        .social-icon svg {
            width: 100%;
            height: 100%;
            fill: var(--section-title-color);
            transition: fill 0.3s;
        }
        .social-icon:hover svg {
            fill: #99ddff;
        }
        .publication .title a {
            color: #fff;
            text-decoration: underline;
        }
        .recent-updates {
            margin-top: 40px;
            margin-bottom: 40px;
        }
        .recent-updates h2 {
            color: var(--section-title-color);
            margin-bottom: 20px;
        }
        .update-item {
            margin-bottom: 15px;
            padding: 10px 0;
            border-bottom: 1px solid #333;
        }
        .update-item:last-child {
            border-bottom: none;
        }
        .update-date {
            color: var(--name-color);
            font-weight: bold;
            font-size: 0.9em;
        }
        .update-text {
            color: #e0e0e0;
            margin-top: 5px;
        }
    </style>
</head>
<body>
    <header>
        <img src="assets/profile_400x400-.png" alt="Tahereh Toosi" class="profile-img">
        <div class="header-content">
            <h1>Tahereh Toosi, PhD</h1>
            <p>
                Associate Research Scientist at the <a href="https://ctn.zuckermaninstitute.columbia.edu/about" target="_blank" style="color: #10b981;">Center for Theoretical Neuroscience</a>, Columbia University. My research bridges computational neuroscience and AI, focusing on building intrinsically aligned models of visual perception. Supported by an <a href="https://reporter.nih.gov/project-details/1K99EY035357-01" target="_blank" style="color: #10b981;">NIH K99/R00</a> award and a grant from <a href="https://arni-institute.org/" target="_blank" style="color: #10b981;">Institute for Artificial and Natural Intelligence (ARNI)</a>, my research leverages AI tools and biological constraints to understand core intelligence.
            </p>
            <div class="social-icons">
                <a href="https://github.com/toosi" target="_blank" class="social-icon" title="GitHub">
                    <svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
                </a>
                <a href="https://twitter.com/taherehtoosi" target="_blank" class="social-icon" title="Twitter">
                    <svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"/></svg>
                </a>
                <a href="https://bsky.app/profile/taherehtoosi.bsky.social" target="_blank" class="social-icon" title="Bluesky">
                    <svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.3723 0 0 5.3723 0 12s5.3723 12 12 12 12-5.3723 12-12S18.6277 0 12 0zm3.7416 16.0004c-.113.0748-.3107.1687-.6107.2527-1.0766.284-2.3818.4366-3.1309.4366-.749 0-2.0542-.1526-3.1308-.4379-.3001-.0827-.4978-.1766-.6108-.2527a.1466.1466 0 0 1-.0628-.12v-2.3543a.1401.1401 0 0 1 .086-.131c.344-.1508.6587-.3297.9668-.5326-.1868-.125-.3616-.24-.527-.3466-1.3054-1.0752-1.9804-2.6215-1.9804-3.3946 0-.9401.6337-2.1857 1.5664-3.041C9.2111 5.327 10.3607 4.8312 12 4.8312c1.6393 0 2.7889.4958 3.6914 1.3477.9327.8553 1.5664 2.1009 1.5664 3.041 0 .7731-.675 2.3194-1.9804 3.3946-.1654.1066-.3402.2216-.527.3466.3055.203.6228.3819.9668.5326.052.02.086.0734.086.131v2.3542c0 .048-.0229.0934-.0628.12z"/></svg>
                </a>
                <a href="https://www.linkedin.com/in/tahereh-toosi-45b99014/" target="_blank" class="social-icon" title="LinkedIn">
                    <svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                </a>
                <!-- <a href="mailto:tahereh.toosi@columbia.edu" class="social-icon" title="Email">
                    <svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M24 5.457v13.909c0 .904-.732 1.636-1.636 1.636h-3.819V11.73L12 16.64l-6.545-4.91v9.273H1.636A1.636 1.636 0 0 1 0 19.366V5.457c0-2.023 2.309-3.178 3.927-1.964L5.455 4.64 12 9.548l6.545-4.91 1.528-1.145C21.69 2.28 24 3.434 24 5.457z"/></svg>
                </a> -->
                <!-- <a href="assets/Tahereh_CV_2025_norefs.pdf" target="_blank" class="social-icon" title="CV">
                    <img src="assets/icons/CV.png" alt="CV" style="width: 24px; height: 24px;">
                </a> -->
            </div>
        </div>
    </header>

    <nav class="nav">
        <a href="#projects" class="active" onclick="showSection('projects')">Research Lines</a>
        <a href="#publications" onclick="showSection('publications')">Publications</a>
        <a href="#talks" onclick="showSection('talks')">Talks</a>
    </nav>

    <section id="projects" class="projects section active">
        <div class="recent-updates">
            <h2>Recent Updates</h2>
            
            <div class="update-item">
                <div class="update-date">August 2025</div>
                <div class="update-text">Paper accepted at Interpreting Cognition in Deep Learning Models Workshop (NeurIPS)</div>
            </div>
            
            <div class="update-item">
                <div class="update-date">August 2025</div>
                <div class="update-text">Paper accepted at Mechanistic Interpretability Workshop (NeurIPS)</div>
            </div>
            
            <div class="update-item">
                <div class="update-date">July 2025</div>
                <div class="update-text">Awarded a grant from <a href="https://arni-institute.org/" target="_blank" style="color: #10b981;">Institute for Artificial and Natural Intelligence (ARNI)</a> on Modular Computations in AI and Neuroscience: Principles and Applications</div>
            </div>
        </div>
        
        <h2>Research Lines</h2>
        <div class="project">
            <h3><a href="research/generative-inference.html">Neural basis of perception</a></h3>
            <h4>Understanding how neural networks give rise to perceptual grouping, illusory percepts, and imagination </h4>
            <div class="gif-container">
                <img src="assets/gifs/FaceVase1_movie.gif" alt="Face-Vase Illusion Transformation 1">
                <img src="assets/gifs/FaceVase2_movie.gif" alt="Face-Vase Illusion Transformation 2">
            </div>
            <div class="gif-container">
                <img src="project_generative_inference/kanizsa_demo.gif" alt="Kanitza">
                <img src="project_generative_inference/neon_demo.gif" alt="Neon">
            </div>
            <div class="gif-container">

                <img src="project_generative_inference/gestalt_demo.gif" alt="Gestalt" style="width: 410px;">
            </div>
            <div class="gif-container">
                <img src="project_generative_inference/imagination_demo_combined_slow.gif" alt="Imagination" style="width: 410px;">
            </div>
            <div class="tags">
                <span class="tag neuroscience">Neuroscience</span>
                <span class="tag ai">AI</span>
            </div>
        </div>

        <!-- <div class="project">
            <h3><a href="research/mechanistic_interpretability.html">Interpretability at the Network Level: Prior-Guided Drift Diffusion for Neural Circuit Analysis</a></h3>
            <h4>Understanding what concepts networks have learned by treating them as generative models to probe their learned statistical priors</h4>
            <div class="gif-container">
                <img src="project_generative_inference/imagination_demo_combined_slow.gif" alt="PGDD Network Interpretability" style="width: 410px;">
            </div>
            <div class="tags">
                <span class="tag neuroscience">Neuroscience</span>
                <span class="tag ai">AI</span>
            </div>
        </div> -->
        
        <div class="project">
            <h3><a href="research/feedback-feedforward.html">Bio-plausible learning algorithms</a></h3>
            <h4>Realisitc alternatives to backpropagation of errors</h4>
            
            <img src="assets/gifs/tweets_FFA.gif" alt="Feedback-Feedforward Alignment GIF" style="width:100%;max-width:600px;height:auto;margin:10px 0;" />
            <div class="tags">
                <span class="tag neuroscience">Neuroscience</span>
                <span class="tag ai">AI</span>
            </div>
        </div>
        
        <div class="project">
            <h3><a href="research/straightened-representations.html">Emergence of temporally predictive representations in robust neural networks</a></h3>
            <h4>Robustness to input noise leads to emergent temporal predictability in neural representations</h4>
            <!-- <p>In this project, we explore how robust training techniques foster the emergence of straightened feature representations in feedforward neural networks when processing natural movie sequences. By leveraging adversarial robustness and random smoothing, our models exhibit a notable decrease in curvature within their latent spaces—allowing for linear interpolation of temporal frames and reliable frame reconstruction. These straightened representations align with perceptual phenomena observed in human vision and demonstrate improved predictivity of neural activity in primary visual cortex (V1). Our results suggest that noise robustness can serve as a parsimonious and biologically plausible mechanism for generating temporal predictability in visual representations.</p> -->
            <img src="assets/gifs/temporal.png" alt="Temporal Representations Preview" style="width:100%;max-width:800px;display:block;margin:20px auto 0 auto;border-radius:8px;box-shadow:0 2px 8px rgba(0,0,0,0.3);" />
            <div class="tags">
                <span class="tag neuroscience">Neuroscience</span>
                <span class="tag ai">AI</span>
            </div>
        </div>

        <div class="project">
            <h3><a href="research/interpretable-features.html">Interpretable features to identify neural representations</a></h3>
            <h4>Making neural representations (natural or artificial) interpretable using meta-space analysis</h4>
            <img src="assets/gifs/MSA_preview.png" alt="MSA interpretable features preview" style="width:100%;max-width:800px;display:block;margin:20px auto 0 auto;border-radius:8px;box-shadow:0 2px 8px rgba(0,0,0,0.3);" />
            <div class="tags">
                <span class="tag neuroscience">Neuroscience</span>
                <span class="tag ai">AI</span>
            </div>
        </div>

        <div class="project">
            <h3><a href="research/temporal-attention.html">Experiments on Humans and non-human primates: Temporal Attention and object recognition abilities</a></h3>
            <h4>Understanding how the brain uses temporal patterns to anticipate and shape perception</h4>
            <!-- <p>In this work, I investigate how the human brain extracts sub-second temporal patterns from repeated or structured contexts to anticipate upcoming stimuli. Combining EEG, psychophysical tasks, and continuous flash suppression paradigms, I examine how alpha oscillations and cortical excitability are shaped by learned temporal associations. These experiments reveal how predictive signals can enhance or suppress conscious perception, offering new insights into the fundamental role of time-based cues in shaping attention and perceptual thresholds.</p> -->
            <img src="assets/gifs/JNP2017.jpeg" alt="Temporal Attention EEG Results" style="width:100%;max-width:400px;display:block;margin:20px auto 0 auto;border-radius:8px;box-shadow:0 2px 8px rgba(0,0,0,0.3);" />
            <div class="tags">
                <span class="tag neuroscience">Neuroscience</span>
            </div>
        </div>
    </section>
    
    <section id="publications" class="publications section">
        <h2>Publications</h2>
        
        <h3>Journals, Proceedings, and Preprints</h3>
        
        <div class="publication">
            <p class="title">Interpretability at the Network Level: Prior-Guided Drift Diffusion for Neural Circuit Analysis</p>
            <p class="authors">Toosi, T., & Miller, K. D.</p>
            <p class="venue">Mechanistic Interpretability Workshop, Neural Information Processing Systems (NeurIPS) (2025)</p>
        </div>
        
        <div class="publication">
            <p class="title">Unifying Gestalt Principles Through Inference-Time Prior Integration</p>
            <p class="authors">Toosi, T., & Miller, K. D.</p>
            <p class="venue">Interpreting Cognition in Deep Learning Models Workshop, Neural Information Processing Systems (NeurIPS) (2025)</p>
        </div>
        
        <div class="publication">
            <p class="title">Illusions as features: the generative side of recognition</p>
            <p class="authors">Toosi, T., & Miller, K. D.</p>
            <p class="venue">Workshop on Scientific Methods for Understanding Deep Learning, Advances in Neural Information Processing Systems (NeurIPS) (2024)</p>
        </div>
        
        <div class="publication">
            <p class="title"><a href="https://papers.nips.cc/paper_files/paper/2023/file/b29ec434e049fb96f3c4245a405ee976-Supplemental-Conference.pdf" target="_blank" style="color:#fff;text-decoration:underline;">Brain-like flexible visual inference by harnessing feedback-feedforward alignment</a></p>
            <p class="authors">Toosi, T., & Issa, E. B.</p>
            <p class="venue">Advances in Neural Information Processing Systems (NeurIPS) (2023)</p>
        </div>
        
        <div class="publication">
            <p class="title"><a href="https://arxiv.org/abs/2308.13870" target="_blank" style="color:#fff;text-decoration:underline;">Brain-like representational straightening of natural movies in robust feedforward neural networks</a></p>
            <p class="authors">Toosi, T., & Issa, E. B.</p>
            <p class="venue">International Conference on Learning Representations (ICLR) (2023)</p>
        </div>
        
        <div class="publication">
            <p class="title">Representational constraints underlying similarity between task-optimized neural systems</p>
            <p class="authors">Toosi, T.</p>
            <p class="venue">arXiv (2023)</p>
        </div>
        
        <div class="publication">
            <p class="title">Marmoset core visual object recognition behavior is comparable to that of macaques and humans</p>
            <p class="authors">Kell, A. J. E., Bokor, S., Jeon, Y., Toosi, T., & Issa, E. B.</p>
            <p class="venue">iScience (2023)</p>
        </div>
        
        <div class="publication">
            <p class="title">Learning temporal context enhances the prestimulus alpha oscillations in the parietal cortex and improves the visual discrimination performance</p>
            <p class="authors">Toosi, T., Tousi, E. K., & Esteky, H.</p>
            <p class="venue">Journal of Neurophysiology (2016)</p>
        </div>
        
        <h3>Recent Peer Reviewed Abstracts</h3>
        
        <div class="publication">
            <p class="title">A unified computational framework for visual dysfunctions in psychosis</p>
            <p class="authors">Toosi, T., & Miller, K. D.</p>
            <p class="venue">Journal of Vision (2025)</p>
        </div>
        
        <div class="publication">
            <p class="title">Generative inference in object recognition models—A unifying framework for discriminative and generative computations in vision</p>
            <p class="authors">Toosi, T., & Miller, K. D.</p>
            <p class="venue">From Neuroscience to Artificially Intelligent Systems (2024)</p>
        </div>
        
        <div class="publication">
            <p class="title">Generative perceptual inference in deep neural network models of object recognition induces illusory contours and shapes</p>
            <p class="authors">Toosi, T., & Miller, K. D.</p>
            <p class="venue">Cognitive Computational Neuroscience (CCN) (2024)</p>
        </div>
        
        <div class="publication">
            <p class="title">Emergence of illusory contours in robust deep neural networks by accumulation of implicit priors</p>
            <p class="authors">Toosi, T., & Miller, K. D.</p>
            <p class="venue">Computational and Systems Neuroscience (Cosyne) (2024)</p>
        </div>
        
        <div class="publication">
            <p class="title">Representational constraints underlying similarity between task-optimized neural systems</p>
            <p class="authors">Toosi, T.</p>
            <p class="venue">Unifying Representations in Neural Models Workshop, Neural Information Processing Systems (NeurIPS) (2023)</p>
        </div>
        
        <div class="publication">
            <p class="title">Object-enhanced and object-centered representations across primate ventral visual cortex</p>
            <p class="authors">Toosi, T., Kriegeskorte, N., & Issa, E. B.</p>
            <p class="venue">Cognitive Computational Neuroscience (CCN) (2023)</p>
        </div>
        
        <div class="publication">
            <p class="title">Perceptually-aligned gradients by sampling the implicit prior</p>
            <p class="authors">Toosi, T.</p>
            <p class="venue">Conference on the Mathematical Theory of Deep Neural Networks (DeepMath) (2022)</p>
        </div>
    </section>

    <section id="talks" class="talks section">
        <h2>Invited Talks</h2>
        
        <div class="publication">
            <p class="title">Generative inference in object recognition models—A unifying framework for discriminative and generative computations in vision</p>
            <p class="venue">TigerBrain Research Symposium, Princeton University (2024)</p>
        </div>

        <div class="publication">
            <p class="title">Generative inference in object recognition models—A unifying framework for discriminative and generative computations in vision</p>
            <p class="venue">From Neuroscience to Artificially Intelligent Systems, Cold Spring Harbor Laboratory (2024)</p>
        </div>

        <div class="publication">
            <p class="title">Generative perceptual inference in deep neural network models of object recognition induces illusory shapes</p>
            <p class="venue">Swartz Foundation Meeting, University of Washington (2024)</p>
        </div>

        <div class="publication">
            <p class="title">Emergence of illusory contours in robust deep neural networks by accumulation of implicit priors</p>
            <p class="venue">Object Recognition: Models, Vision Science Society Meeting, St. Pete Beach Florida (2024)</p>
        </div>

        <div class="publication">
            <p class="title">Cortical computations underlying the integration of perceptual priors and sensory processing</p>
            <p class="venue">Brain Science External Postdoc Seminar Series, Brown University (2024)</p>
        </div>

        <div class="publication">
            <p class="title">Can images predict neural patterns better than Deep Nets?</p>
            <p class="venue">ICBINB Workshop, Cosyne Meeting, Lisbon, Portugal (2024)</p>
        </div>

        <div class="publication">
            <p class="title">Harnessing feedback pathways: Integrating perceptual priors in sensory processing</p>
            <p class="venue">SYNAPSES Seminar Series, Yale University (2024)</p>
        </div>

        <div class="publication">
            <p class="title">Uncovering the evolution of neural representations in the ventral visual stream</p>
            <p class="venue">Neuroscience and Artificial Intelligence Laboratory (NeuroAILab), Stanford University (2023)</p>
        </div>

        <div class="publication">
            <p class="title">Interpretable intermediate representations in primate ventral visual cortex</p>
            <p class="venue">Visual Inference Lab, Columbia University (2023)</p>
        </div>

        <div class="publication">
            <p class="title">Representational straightening of natural movies in robust feedforward neural networks</p>
            <p class="venue">Visual Object and Scene Recognition Nanosymposium, Society for Neuroscience Meeting, San Diego (2022)</p>
        </div>

        <div class="publication">
            <p class="title">Symbiotic learning of feedforward and feedback networks</p>
            <p class="venue">From Neuroscience to Artificially Intelligent Systems, Cold Spring Harbor Laboratory, 2020</p>
        </div>
    </section>
    <script>
        function showSection(sectionId) {
            // Hide all sections
            document.querySelectorAll('.section').forEach(function(section) {
                section.classList.remove('active');
            });
            
            // Show the selected section
            document.getElementById(sectionId).classList.add('active');
            
            // Update active nav item
            document.querySelectorAll('.nav a').forEach(function(navItem) {
                navItem.classList.remove('active');
                if (navItem.getAttribute('href') === '#' + sectionId) {
                    navItem.classList.add('active');
                }
            });
        }

        // Visitor counter for all visitors
        async function updateCounter() {
            try {
                // Get current count
                const getResponse = await fetch('https://api.countapi.xyz/get/toosi.github.io/visits');
                const getData = await getResponse.json();
                
                // Update count
                const updateResponse = await fetch('https://api.countapi.xyz/hit/toosi.github.io/visits');
                const updateData = await updateResponse.json();
                
                // Create or update counter display
                let counterDisplay = document.getElementById('visitor-counter');
                if (!counterDisplay) {
                    counterDisplay = document.createElement('div');
                    counterDisplay.id = 'visitor-counter';
                    counterDisplay.style.position = 'fixed';
                    counterDisplay.style.bottom = '20px';
                    counterDisplay.style.right = '20px';
                    counterDisplay.style.backgroundColor = 'rgba(0,0,0,0.8)';
                    counterDisplay.style.color = '#80cbc4';
                    counterDisplay.style.padding = '8px 15px';
                    counterDisplay.style.borderRadius = '8px';
                    counterDisplay.style.fontSize = '14px';
                    counterDisplay.style.zIndex = '1000';
                    counterDisplay.style.fontFamily = "'Open Sans', Verdana, sans-serif";
                    counterDisplay.style.boxShadow = '0 2px 4px rgba(0,0,0,0.2)';
                    document.body.appendChild(counterDisplay);
                }
                counterDisplay.textContent = `Total Visits: ${updateData.value}`;
            } catch (error) {
                console.log('Counter error:', error);
            }
        }

        // Initialize counter when the page loads
        document.addEventListener('DOMContentLoaded', updateCounter);
    </script>
</body>
</html>