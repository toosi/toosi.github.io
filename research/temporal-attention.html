<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Temporal Attention & Predictive Visual Processing - Tahereh Toosi</title>
    <style>
        :root {
            /* Light theme (default) */
            --name-color: #fc5203;         /* Warm orange-red */
            --section-title-color: #006699; /* Teal-blue */
            --text-primary: #1e293b;        /* Slate-800 */
            --text-secondary: #475569;      /* Slate-600 */
            --text-tertiary: #64748b;       /* Slate-500 */
            --accent-color: #0088aa;        /* Teal-blue accent */
            --border-color: #e2e8f0;        /* Slate-200 */
            --bg-subtle: #f8fafc;          /* Slate-50 */
            --bg-primary: #ffffff;         /* White background */
            --bg-secondary: #f9f9f9;       /* Light gray background */
            --text-body: #333333;          /* Dark text */
            --link-color: #5b8a9f;         /* Muted teal-blue for links */
        }
        
        /* Dark theme */
        @media (prefers-color-scheme: dark) {
            :root {
                --name-color: #ff6b3d;         /* Brighter orange-red for dark theme */
                --section-title-color: #4dd0e1; /* Lighter teal-blue */
                --text-primary: #e2e8f0;        /* Light slate */
                --text-secondary: #cbd5e1;      /* Lighter gray */
                --text-tertiary: #94a3b8;       /* Medium gray */
                --accent-color: #5dd3e8;        /* Brighter teal accent */
                --border-color: #334155;        /* Dark slate borders */
                --bg-subtle: #1e293b;          /* Dark slate background */
                --bg-primary: #0f172a;         /* Very dark blue background */
                --bg-secondary: #1e293b;       /* Dark slate for cards */
                --text-body: #e2e8f0;          /* Light text */
                --link-color: #5dd3e8;          /* Brighter link color for dark theme */
            }
        }
        
        body {
            font-family: 'Open Sans', Verdana, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: var(--text-body);
            background-color: var(--bg-primary);
            transition: background-color 0.3s ease, color 0.3s ease;
        }
        .back-link {
            display: inline-block;
            margin-bottom: 20px;
            color: var(--section-title-color);
            text-decoration: none;
            transition: color 0.2s ease;
        }
        .back-link:hover {
            text-decoration: underline;
            color: var(--accent-color);
        }
        h1, h2, h3 {
            font-family: 'Montserrat', Arial, sans-serif;
            color: var(--section-title-color);
        }
        h1 {
            color: var(--name-color);
        }
        a {
            color: var(--section-title-color);
            text-decoration: none;
            transition: color 0.2s ease;
        }
        a:hover {
            text-decoration: underline;
            color: var(--accent-color);
        }
        p, li {
            color: var(--text-body);
        }
        .publication {
            margin: 20px 0;
            padding: 15px;
            background-color: var(--bg-secondary);
            border-radius: 8px;
            border: 1px solid var(--border-color);
            transition: background-color 0.3s ease, border-color 0.3s ease;
        }
        @media (prefers-color-scheme: dark) {
            .publication {
                box-shadow: 0 2px 4px rgba(0,0,0,0.3);
            }
        }
        .publication .title {
            font-weight: 600;
            color: var(--text-primary);
        }
        .publication .title a {
            color: var(--link-color);
            text-decoration: underline;
        }
        .publication .title a:hover {
            color: var(--accent-color);
        }
        .publication .authors {
            font-style: italic;
            color: var(--text-secondary);
        }
        .publication .venue {
            color: var(--text-tertiary);
        }
        a[style*="color:#fff"] {
            color: var(--link-color) !important;
        }
    </style>
</head>
<body>
    <a href="../index.html" class="back-link">← Back to Home</a>
    
    <h1>Temporal Attention & Predictive Visual Processing</h1>
    
    <h2>Project Overview</h2>
    <p>
        During my Ph.D. and subsequent research, I conducted studies analyzing neural recordings from the Inferotemporal (IT) cortex of macaques to systematically compare object recognition capabilities in humans and non-human primates. At the DiCarlo Lab (MIT), I investigated whether the neural representations in monkey IT could predict human object recognition behavior. I compared two datasets—one involving passive viewing of thousands of high-variation images and the other focusing on a face-gender discrimination task. My analysis demonstrated that face-gender discrimination was statistically indistinguishable from other high-variation object recognition tasks, supporting the hypothesis that high-level human object recognition behavior can be explained by IT neural activity.
    </p>

    <p>
        In related work, I examined the role of background context in object recognition and how it contributes to discrepancies between human behavior and neural decoder models at the image-level granularity. To address this, I collected human behavioral responses via Amazon Mechanical Turk, where participants classified objects embedded in natural images with varying background contexts. I found that when background information was degraded, the alignment between human behavior and neural decoder predictions decreased, suggesting that monkey IT cortex, like the human visual system, relies on background context for object recognition.
    </p>

    <p>
        Early in my postdoctoral research at Columbia University's Issa Lab, I investigated the core object recognition capabilities of the common marmoset, evaluating its potential as a new primate model for high-level visual object recognition. I contributed to the development of a novel home cage system for high-throughput behavioral testing, training two marmosets to perform a simple object recognition task. Our results demonstrated that marmosets exhibit core object recognition behavior comparable to macaques and humans, highlighting their suitability as a valuable model for studying high-level vision.
    </p>

    <h2>Research Methods</h2>
    <ul>
        <li>Electroencephalography (EEG) recordings</li>
        <li>Eye-tracking</li>
        <li>Psychophysical tasks</li>
        <li>Continuous flash suppression paradigms</li>
        <li>Behavioral testing of young marmosets on object recognition tasks</li>
    </ul>

    <h2>Publication</h2>
    
    <div class="publication">
        <p class="title"><a href="https://journals.physiology.org/doi/full/10.1152/jn.00969.2016?rfr_dat=cr_pub++0pubmed&url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org" target="_blank" style="color:#fff;text-decoration:underline;">Learning temporal context enhances the prestimulus alpha oscillations in the parietal cortex and improves the visual discrimination performance</a></p>
        <p class="authors">Toosi, T., Tousi, E. K., & Esteky, H.</p>
        <p class="venue">Journal of Neurophysiology (2016)</p>
    </div>
    <div class="publication">
        <p class="title">Marmoset core visual object recognition behavior is comparable to that of macaques and humans</p>
        <p class="authors">Kell, A. J. E., Bokor, S., Jeon, Y., Toosi, T., & Issa, E. B.</p>
        <p class="venue">iScience (2023)</p>
    </div>




</body>
</html>
